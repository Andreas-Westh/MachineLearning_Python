{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple perceptron\n",
    "https://www.youtube.com/watch?v=-KLnurhX-Pg&list=PLqXS1b2lRpYTpUIEu3oxfhhTuBXmMPppA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general values\n",
    "X_input = [0.1,0.5,0.2]\n",
    "w_weights = [0.4, 0.3, 0.6]\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define activation function\n",
    "def step(weighted_sum):\n",
    "    if weighted_sum > threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04000000000000001\n",
      "0.19\n",
      "0.31\n",
      "output: 0\n"
     ]
    }
   ],
   "source": [
    "# now we can define our perceptron\n",
    "def perceptron():\n",
    "    weighted_sum = 0\n",
    "    for x,w in zip(X_input, w_weights):\n",
    "        weighted_sum += x*w\n",
    "        print(weighted_sum)\n",
    "    return step(weighted_sum)\n",
    "\n",
    "output = perceptron()\n",
    "print(\"output: \" + str(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Entropy Loss Error Function\n",
    "https://www.youtube.com/watch?v=EJRFP3WmS6Q&list=PLqXS1b2lRpYTpUIEu3oxfhhTuBXmMPppA\n",
    "\n",
    "\n",
    "We need to make a distriction in our code, for the output we **want**, since that is not always the output we **get** \n",
    "Desired outpus (Target/y) vs actual output (predection/y^)\n",
    "\n",
    "We can **NEVER** mopdify the actual output, as that is fact, but we can modify our prediction, till it is (or gets close to) being the perfect match with the actual output\n",
    "\n",
    "For this we need to use the Error Function, aka Loss Function, real ones know em as Cost tho on god\n",
    "    It calculated the distance between a given point and the target\n",
    "\n",
    "This part will focus on Cross Entrophy Loss aka Log Loss:\n",
    "$$\n",
    "-\\left( y \\times \\log(\\text{w\\_sum}) + (1 - y) \\times \\log(1 - \\text{w\\_sum}) \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "Incorrectly classified inputs, get a bigger penalty than inputs that were correctly classified\n",
    "\n",
    "Now to calculate the total loss:\n",
    "$$\n",
    "\\frac{\\text{loss}[1] + \\text{loss}[2] + \\text{loss}[\\dots] + \\text{loss}[n]}{n}\n",
    "$$\n",
    "*n being the number of observations*\n",
    "\n",
    "and these 2 funtions combined is:\n",
    "$$\n",
    "\\frac{\\sum\\limits_{i=0}^{n} - \\left( y_i \\times \\log(\\text{w\\_sum}_i) + (1 - y_i) \\times \\log(1 - \\text{w\\_sum}_i) \\right)}{n}\n",
    "$$\n",
    "*i = each of our observations, one at a time, a bit like an i in a for loop* \n",
    "\n",
    "Now to coding this bad boy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# make a df with a weighted sum and y\n",
    "input_data = [(0.26, 1),\n",
    "              (0.20, 0),\n",
    "              (0.48, 1),\n",
    "              (0.30, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.585026652029182\n",
      "0.09691001300805639\n",
      "0.3187587626244128\n",
      "0.1549019599857432\n",
      "error_term: 0.2888993469118486\n"
     ]
    }
   ],
   "source": [
    "# now to define the cross entropy function\n",
    "def cross_entropy(input_data):\n",
    "    loss = 0\n",
    "    n = len(input_data)\n",
    "    for entry in input_data:\n",
    "        w_sum = entry[0]\n",
    "        y = entry[1]\n",
    "        loss += -(y*math.log10(w_sum) + (1-y)*math.log10(1-w_sum))\n",
    "        print(-(y*math.log10(w_sum) + (1-y)*math.log10(1-w_sum))) # just to see the individual loss value\n",
    "    return loss/n\n",
    "\n",
    "error_term = cross_entropy(input_data)\n",
    "print(\"error_term: \" + str(error_term))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent\n",
    "https://www.youtube.com/watch?v=jwStsp8JUPU&list=PLqXS1b2lRpYTpUIEu3oxfhhTuBXmMPppA\n",
    "\n",
    "Used for:\n",
    "1. updating the weights of AI models\n",
    "2. minimizing the training loss\n",
    "\n",
    "Gradient Descent vs Perceptron\n",
    "    Gradient Descent recieves decimal prediction (sigmoid function)\n",
    "    Perceptron provides binary prediction (step function)\n",
    "\n",
    "Sigmoid function:\n",
    "$$\n",
    "\\sigma(\\text{w\\_sum}) = \\frac{1}{1 + e^{-\\text{w\\_sum}}}\n",
    "$$\n",
    "\n",
    "Since this doesnt include a threshold, we will add into into our Weighted Sum\n",
    "$$\n",
    "Weighted Sum = features \\cdot weights + bias (the threshold, so could be 0.5)\n",
    "$$\n",
    "\n",
    "After adding it to the sigmoid function,the result (our prediction) can be added into the Cross Entropy Loss formula\n",
    "\n",
    "Modifications:\n",
    "1. updating the weights:\n",
    "$$\n",
    "New Weight = old weight + learn rate \\cdot (target - pred) \\cdot x[i]\n",
    "$$\n",
    "*the learning weight is a very low number, which maes sure there's a gradual weight update, without any drastic changes*\n",
    "it often appears as:\n",
    "$$ \n",
    "\\alpha\n",
    "$$\n",
    "\n",
    "now for the actual formula:\n",
    "$$\n",
    "w'_{[i]} = w_{[i]} + \\alpha \\cdot (y - \\hat{y}) \\cdot x_{[i]}\n",
    "$$\n",
    "\n",
    "2. New bias\n",
    "$$\n",
    "New Bias = old bias + learn rate \\cdot (target - pred)\n",
    "$$\n",
    "\n",
    "or in math terms:\n",
    "$$\n",
    "b' = b + \\alpha \\cdot (y - \\hat{y})\n",
    "$$\n",
    "The bias is now no longer a solid threshold, but more of a constant value, which helps us tune our model\n",
    "\n",
    "Now our loss should decrease, everytime we interate over our full set of features (this process is also called an Epoch)\n",
    "    The average loss is how we measure the performance of our model\n",
    "\n",
    "Now it's time to get down and dirty with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation function\n",
    "def sigmoid(w_sum):\n",
    "    return 1/(1+np.exp(-w_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction\n",
    "def predict(features, weights, bias):\n",
    "    return sigmoid(np.dot(features, weights) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def cross_entropy(target, pred):\n",
    "    return -(target*np.log10(pred)+(1-target)*(np.log10(1-pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update weights\n",
    "def gradient_descent(x, y, weights, bias, learnrate, pred):\n",
    "    new_weights = [] # empty list for our new weights\n",
    "    bias += learnrate*(y-pred)\n",
    "    \n",
    "    for w,xi in zip(weights,x):\n",
    "        new_weight = w + learnrate * (y-pred) * xi\n",
    "        new_weights.append(new_weight)\n",
    "    return new_weights, bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Epoch 0 ==========\n",
      "Average loss:  0.33944294852408824\n",
      "\n",
      "========== Epoch 1 ==========\n",
      "Average loss:  0.33147083470744965\n",
      "\n",
      "========== Epoch 2 ==========\n",
      "Average loss:  0.32499752814643046\n",
      "\n",
      "========== Epoch 3 ==========\n",
      "Average loss:  0.31973828925825093\n",
      "\n",
      "========== Epoch 4 ==========\n",
      "Average loss:  0.3154527992144233\n",
      "\n",
      "========== Epoch 5 ==========\n",
      "Average loss:  0.31194241308286647\n",
      "\n",
      "========== Epoch 6 ==========\n",
      "Average loss:  0.30904555254773175\n",
      "\n",
      "========== Epoch 7 ==========\n",
      "Average loss:  0.3066324010104734\n",
      "\n",
      "========== Epoch 8 ==========\n",
      "Average loss:  0.30459963557320424\n",
      "\n",
      "========== Epoch 9 ==========\n",
      "Average loss:  0.30286560086854775\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "features = np.array(([0.1,0.5,0.2],[0.2,0.3,0.1],[0.7,0.4,0.2],[0.1,0.4,0.3]))\n",
    "targets = np.array([0,1,0,1])\n",
    "\n",
    "epochs = 10\n",
    "learnrate = 0.1\n",
    "\n",
    "errors = []\n",
    "weights = np.array([0.4, 0.2, 0.6])\n",
    "bias = 0.5\n",
    "\n",
    "new_weights = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    for x, y in zip(features, targets):\n",
    "        pred = predict(x, weights, bias)\n",
    "        error = cross_entropy(y, pred)\n",
    "        weights, bias = gradient_descent(x, y, weights, bias, learnrate, pred)\n",
    "    \n",
    "    # Printing out the log-loss error on the training set\n",
    "    out = predict(features, weights, bias)\n",
    "    loss = np.mean(cross_entropy(targets, out))\n",
    "    errors.append(loss)\n",
    "    print(\"\\n========== Epoch\", e,\"==========\")\n",
    "    print(\"Average loss: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
